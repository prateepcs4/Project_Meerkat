import os
from glob import glob
import shutil
import numpy as np
from scipy.ndimage import imread

def get_dir(directory):
    if not os.path.exists(directory):
        os.makedirs(directory)
    return directory


def clear_dir(directory):
    """
    Removes all files in the given directory.
    @param directory: The path to the directory.
    """
    for f in os.listdir(directory):
        path = os.path.join(directory, f)
        try:
            if os.path.isfile(path):
                os.unlink(path)
            elif os.path.isdir(path):
                shutil.rmtree(path)
        except Exception as e:
            print(e)

def get_test_frame_dims():
    img_path = glob(os.path.join(TEST_DIR, '*/*'))[0]
    img = imread(img_path, mode='RGB')
    shape = np.shape(img)

    return shape[0], shape[1]

def get_train_frame_dims():
    img_path = glob(os.path.join(TRAIN_DIR, '*/*'))[0]
    img = imread(img_path, mode='RGB')
    shape = np.shape(img)

    return shape[0], shape[1]

# root directory for all data
DATA_DIR = get_dir('/Data/')
# directory of unprocessed training frames
TRAIN_DIR = os.path.join(DATA_DIR, 'Ms_Pacman/Train/')
# directory of unprocessed test frames
TEST_DIR = os.path.join(DATA_DIR, 'Ms_Pacman/Test/')
# Directory of processed training clips.
# hidden so finder doesn't freeze w/ so many files. DON'T USE `ls` COMMAND ON THIS DIR!
TRAIN_DIR_CLIPS = get_dir(os.path.join(DATA_DIR, '.Clips/'))

# For processing clips. l2 diff between frames must be greater than this
MOVEMENT_THRESHOLD = 100
# total number of processed clips in TRAIN_DIR_CLIPS
NUM_CLIPS = len(glob(TRAIN_DIR_CLIPS + '*'))

# the height and width of the full frames to test on. Set in avg_runner.py or process_data.py main.
FULL_HEIGHT = 210
FULL_WIDTH = 160
# the height and width of the patches to train on
TRAIN_HEIGHT = TRAIN_WIDTH = 32


# root directory for all saved content
SAVE_DIR = get_dir('../Save/')
# inner directory to differentiate between runs
SAVE_NAME = 'Default/'
# directory for saved models
MODEL_SAVE_DIR = get_dir(os.path.join(SAVE_DIR, 'Models/', SAVE_NAME))
# directory for saved TensorBoard summaries
SUMMARY_SAVE_DIR = get_dir(os.path.join(SAVE_DIR, 'Summaries/', SAVE_NAME))
# directory for saved images
IMG_SAVE_DIR = get_dir(os.path.join(SAVE_DIR, 'Images/', SAVE_NAME))

STATS_FREQ      = 10     # how often to print loss/train error stats, in # steps
SUMMARY_FREQ    = 100    # how often to save the summaries, in # steps
IMG_SAVE_FREQ   = 1000   # how often to save generated images, in # steps
TEST_FREQ       = 5000   # how often to test the model on test data, in # steps
MODEL_SAVE_FREQ = 10000  # how often to save the model, in # steps

# the training minibatch size
BATCH_SIZE = 8
# the number of history frames to give as input to the network
HIST_LEN = 4
# the number of frames generated by the network
OUT_LEN = 2

##
# Generator model
##

# learning rate for the generator model
LRATE_G = 0.00004
# padding for convolutions in the generator model
PADDING_G = 'SAME'
# feature maps for each convolution of the network in the generator model
FMAPS_G = [3 * HIST_LEN, 128, 256, 128, 3]
# kernel sizes for each convolution of each scale network in the generator model
KERNEL_SIZES_G = [3, 3, 3, 3]


##
# Discriminator model
##

PADDING_D = 'SAME'
LRATE_D = 0.01
# feature maps for each convolution of the network in the discriminator model
FMAPS_D = [3, 64, 128]
# kernel sizes for each convolution of each scale network in the discriminator model
KERNEL_SIZES_D = [3, 3]
# layer sizes for each fully-connected layer of each scale network in the discriminator model
# layer connecting conv to fully-connected is dynamically generated when creating the model
FC_LAYER_SIZES_D = [512, 256, 1]